{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RFM Analysis with PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x_data = pd.read_csv('C:/Users/Gajraj Singh/Desktop/LTV1-Namashi.csv')\n",
    "#spark code for read_csv\n",
    "# data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"C:/Users/Gajraj Singh/Desktop/LTV1-Namashi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>isattr</th>\n",
       "      <th>eventItemJson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"locale\":\"ar_SA\",\"criteo_p\":\"%5B%7B%22i%22:%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"device_name\":\"iPhone\",\"locale\":\"en_AE\",\"fb_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"revenue\":\"27\",\"fb_value\":\"27\",\"device_name\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"transaction_id\":\"SA7A7066474\",\"fb_currency\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-10-18</td>\n",
       "      <td>False</td>\n",
       "      <td>{\"fb_content\":\"[{\\\"id\\\":\\\"GL103AT84LOD\\\",\\\"qua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceDate  isattr                                      eventItemJson\n",
       "0  2019-10-18   False  {\"locale\":\"ar_SA\",\"criteo_p\":\"%5B%7B%22i%22:%2...\n",
       "1  2019-10-18   False  {\"device_name\":\"iPhone\",\"locale\":\"en_AE\",\"fb_c...\n",
       "2  2019-10-18   False  {\"revenue\":\"27\",\"fb_value\":\"27\",\"device_name\":...\n",
       "3  2019-10-18   False  {\"transaction_id\":\"SA7A7066474\",\"fb_currency\":...\n",
       "4  2019-10-18   False  {\"fb_content\":\"[{\\\"id\\\":\\\"GL103AT84LOD\\\",\\\"qua..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "x_data['InvoiceNo'] = x_data['eventItemJson'].apply(lambda x:json.loads(x).get('transaction_id','NULL'))\n",
    "x_data['StockCode'] = x_data['eventItemJson'].apply(lambda x:json.loads(x).get('g_sku','NULL'))\n",
    "x_data['CustomerID'] = x_data['eventItemJson'].apply(lambda x:json.loads(x).get('user_id','NULL'))\n",
    "x_data['Revenue'] = x_data['eventItemJson'].apply(lambda x:json.loads(x).get('revenue','NULL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.drop(['eventItemJson'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1101897 entries, 0 to 1101896\n",
      "Data columns (total 6 columns):\n",
      "InvoiceDate    1101897 non-null object\n",
      "isattr         1101897 non-null bool\n",
      "InvoiceNo      1101897 non-null object\n",
      "StockCode      1101897 non-null object\n",
      "CustomerID     1101897 non-null object\n",
      "Revenue        1101897 non-null object\n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 43.1+ MB\n"
     ]
    }
   ],
   "source": [
    "x_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "data =sqlCtx.createDataFrame(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- isattr: boolean (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Revenue: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "data = data.withColumn(\"Revenue\", data[\"Revenue\"].cast(DoubleType()))\n",
    "# data = data.withColumn(\"InvoiceDate\", to_date(unix_timestamp(\"InvoiceDate\", \"MM/dd/yyyy\").cast(\"timestamp\")))\n",
    "data.select('InvoiceDate',from_unixtime(unix_timestamp('InvoiceDate', '%M/%d/%y')).alias('InvoiceDate'))\n",
    "\n",
    "\n",
    "# define Total column\n",
    "# data = data.withColumn(\"Total\", round(data[\"UnitPrice\"] * data[\"Quantity\"], 2))\n",
    "\n",
    "\n",
    "# calculate difference in days between 2011-12-31 and the Invoice Date\n",
    "data = data.withColumn(\"RecencyDays\", expr(\"datediff('2020-01-15', InvoiceDate)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------+--------------------+----------+-------+-----------+\n",
      "|InvoiceDate|isattr|  InvoiceNo|           StockCode|CustomerID|Revenue|RecencyDays|\n",
      "+-----------+------+-----------+--------------------+----------+-------+-----------+\n",
      "| 2019-10-18| false|SA7A7882559|   \"72704SH52LQP-9S\"|   5982454|  121.0|         89|\n",
      "| 2019-10-18| false|AE7A9279439|\"SW493AC30KBX-929...|   7476848|  104.0|         89|\n",
      "| 2019-10-18| false|AE7A5386753|\"CA787AT17WJI-152...|   2702676|   27.0|         89|\n",
      "| 2019-10-18| false|SA7A7066474|\"FO436AC46PJL-150...|   5072903|  110.0|         89|\n",
      "| 2019-10-18| false|AE7A1635365|\"GL103AT84LOD-107...|   5571728|   16.0|         89|\n",
      "+-----------+------+-----------+--------------------+----------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_table = data.groupBy(\"CustomerId\").agg(min(\"RecencyDays\").alias(\"Recency\"),\\\n",
    "                                           count(\"InvoiceNo\").alias(\"Frequency\"),sum(\"Revenue\").alias(\"Monetary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CustomerId='NULL', Recency=0, Frequency=110431, Monetary=12712381.0),\n",
       " Row(CustomerId='2859680', Recency=0, Frequency=191, Monetary=32938.0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rfm_table.count()\n",
    "rfm_table.orderBy(desc(\"Frequency\")).take(2)\n",
    "# rfm_table.agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_table = rfm_table.withColumn(\"F\", \\\n",
    "                                 when(col(\"Monetary\") >= F[2] , 4).\\\n",
    "                                 when(col(\"Monetary\") >= F[1] , 3).\\\n",
    "                                 when(col(\"Monetary\") >= F[0] , 2).\\\n",
    "                                 otherwise(1))\n",
    "\n",
    "\n",
    "\n",
    "# R = rfm_table.approxQuantile(\"Recency\", [0.25, 0.5, 0.75], 0)\n",
    "# F = rfm_table.approxQuantile(\"Frequency\", [0.25, 0.5, 0.75], 0)\n",
    "# M = rfm_table.approxQuantile(\"Monetary\", [0.25, 0.5, 0.75], 0)\n",
    "\n",
    "# rfm_table = rfm_table.withColumn(\"R\", \\\n",
    "#                                  when(col(\"Recency\") >= R[2] , 1).\\\n",
    "#                                  when(col(\"Recency\") >= R[1] , 2).\\\n",
    "#                                  when(col(\"Recency\") >= R[0] , 3).\\\n",
    "#                                  otherwise(4))\n",
    "\n",
    "# rfm_table = rfm_table.withColumn(\"F\", \\\n",
    "#                                  when(col(\"Monetary\") >= F[2] , 4).\\\n",
    "#                                  when(col(\"Monetary\") >= F[1] , 3).\\\n",
    "#                                  when(col(\"Monetary\") >= F[0] , 2).\\\n",
    "#                                  otherwise(1))\n",
    "\n",
    "# rfm_table = rfm_table.withColumn(\"M\", \\\n",
    "#                                  when(col(\"Monetary\") >= M[2] , 4).\\\n",
    "#                                  when(col(\"Monetary\") >= M[1] , 3).\\\n",
    "#                                  when(col(\"Monetary\") >= M[0] , 2).\\\n",
    "#                                  otherwise(1))\n",
    "\n",
    "# rfm_table = rfm_table.withColumn(\"RFM_Score\", concat(col(\"R\"), col(\"F\"), col(\"M\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 2.0]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_rfmscore = rfm_table.groupBy(\"RFM_Score\").count().orderBy(\"count\", ascending=False)\n",
    "#converting data to pandas\n",
    "grouped_by_rfmscore_pandas = grouped_by_rfmscore.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_rfmscore_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a4_dims = (20, 9)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "ax = sns.barplot(x='RFM_Score', y='count', data=grouped_by_rfmscore_pandas,saturation=7)\n",
    "ax.set(xlabel='RFM_Score', ylabel='Number of Customer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Below are some segments:\n",
    "1. High-spending New Customers — (4–1–3 and 4–1–4).These are customers who transacted only once, but very recently and they spent a lot.\n",
    "2. Lowest-Spending Active Loyal Customers.(3–4–1 and 4–4–1). They transacted recently and do so often, but spend the least.\n",
    "3. Churned Best Customers.(1–3–3, 1–4–3, 1–3–4 and 1–4–4). They transacted frequently and spent a lot, but it’s been a long time since they’ve transacted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_schema = StructType([ \n",
    "     StructField('CustomerId', DoubleType(), True),\n",
    "    StructField('R', IntegerType(), True),\n",
    "    StructField('F', IntegerType(), True),\n",
    "    StructField('M', IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "user_schema = StructType([ \n",
    "     StructField('CustomerId', DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "spend_schema = StructType([ \n",
    "     StructField('CustomerId', DoubleType(), True),\n",
    "     StructField('Monetary', DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "# Load R,F,M data for shoppers in the train set\n",
    "cluster_data = rfm_table.select([c for c in rfm_table.columns if c in ['CustomerId','R','F','M']])\n",
    "cluster_data.createOrReplaceTempView(\"train\")\n",
    "# Load users who have purchased in the test set\n",
    "user_data = rfm_table.select([c for c in rfm_table.columns if c in ['CustomerId']])\n",
    "user_data.createOrReplaceTempView(\"test\")\n",
    "# Load avg spend (in the time period) of shoppers\n",
    "spend_data = rfm_table.select([c for c in rfm_table.columns if c in ['CustomerId','Monetary']])\n",
    "spend_data.createOrReplaceTempView(\"spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''\n",
    "    select DISTINCT A.*, \n",
    "       CASE WHEN B.CustomerId IS NOT NULL\n",
    "       THEN 1\n",
    "       ELSE 0\n",
    "       END Buy\n",
    "    from train A\n",
    "    left join test B\n",
    "    on A.CustomerId = B.CustomerId\n",
    "'''\n",
    "final_df = spark.sql(q)\n",
    "final_df.createOrReplaceTempView(\"final_df\")\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of shoppers\n",
    "total_shoppers_q = '''\n",
    "SELECT COUNT(*) FROM final_df\n",
    "'''\n",
    "spark.sql(total_shoppers_q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df = spark.sql('''\n",
    "SELECT ROUND(COUNT(*)/457958,4) as pct, R FROM final_df WHERE Buy == 1 GROUP BY R''')\n",
    "F_df = spark.sql('''\n",
    "SELECT ROUND(COUNT(*)/457958,4) as pct, F FROM final_df WHERE Buy == 1 GROUP BY F''')\n",
    "M_df = spark.sql('''\n",
    "SELECT ROUND(COUNT(*)/457958,4) as pct, M FROM final_df WHERE Buy == 1 GROUP BY M''')\n",
    "# Let's look at what the M dataframe looks like\n",
    "M_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for plotting sake\n",
    "R_df.toPandas().plot(x=\"R\", y=\"pct\", kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for plotting sake\n",
    "F_df.toPandas().plot(x=\"F\", y=\"pct\", kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for plotting sake\n",
    "M_df.toPandas().plot(x=\"M\", y=\"pct\", kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# Data\n",
    "data = final_df.withColumnRenamed(\"Buy\", \"label\")\n",
    "training_set, testing_set = data.randomSplit([0.75,0.25])\n",
    "\n",
    "# Model and pipeline\n",
    "lr = LogisticRegression()\n",
    "assembler = VectorAssembler(inputCols = [\"R\",\"F\",\"M\"], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[assembler,lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "fitted_pipeline = pipeline.fit(training_set)\n",
    "\n",
    "# Get model from the fitted pipeline\n",
    "lrModel = fitted_pipeline.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model\n",
    "# The probability value is their retention rate\n",
    "predictions = lrModel.transform(assembler.transform(testing_set)).select(\"CustomerId\", \"probability\")\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondelement=udf(lambda v:float(v[1]),FloatType())\n",
    "predictions = (predictions.select(secondelement('probability'), \"CustomerId\")\n",
    "        .withColumnRenamed(\"<lambda>(probability)\", \"retention_rate\"))\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = predictions.join(spend_data, how=\"inner\", on =\"CustomerId\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_clv(df, num_periods, discount):\n",
    "\n",
    "    clv = lambda num_periods, retention, spend, discount: sum(\n",
    "        [spend*(retention/(1 + discount))**t for t in range(1, num_periods + 1)])\n",
    "    clv_rdd = (df.rdd.map(lambda x: (x[\"CustomerId\"],\n",
    "                                     round(clv(num_periods, x[\"retention_rate\"],\n",
    "                                           x[\"Monetary\"], discount), 2))))\n",
    "\n",
    "    clv_df = clv_rdd.toDF().withColumnRenamed(\"_1\", \"CustomerId\").withColumnRenamed(\"_2\", \"CLV\")\n",
    "\n",
    "    return clv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifetime = 10\n",
    "discount = 0.1\n",
    "clv_df = calc_clv(data, 10, 0.1) \n",
    "clv_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
